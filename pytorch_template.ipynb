{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-09T14:44:46.651592Z",
     "iopub.status.busy": "2025-09-09T14:44:46.651360Z",
     "iopub.status.idle": "2025-09-09T14:45:01.857892Z",
     "shell.execute_reply": "2025-09-09T14:45:01.857256Z",
     "shell.execute_reply.started": "2025-09-09T14:44:46.651573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import timm\n",
    "import torch\n",
    "import zipfile,os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:45:01.858829Z",
     "iopub.status.busy": "2025-09-09T14:45:01.858499Z",
     "iopub.status.idle": "2025-09-09T14:45:12.507495Z",
     "shell.execute_reply": "2025-09-09T14:45:12.506707Z",
     "shell.execute_reply.started": "2025-09-09T14:45:01.858812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\n",
      "  Downloading roboflow-1.2.7-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.6.15)\n",
      "Collecting idna==3.7 (from roboflow)\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
      "Collecting pi-heif<2 (from roboflow)\n",
      "  Downloading pi_heif-1.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting pillow-avif-plugin<2 (from roboflow)\n",
      "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
      "Collecting python-dotenv (from roboflow)\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
      "Collecting filetype (from roboflow)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Downloading roboflow-1.2.7-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.1.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: pillow-avif-plugin, filetype, python-dotenv, pi-heif, idna, opencv-python-headless, roboflow\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.11.0.86\n",
      "    Uninstalling opencv-python-headless-4.11.0.86:\n",
      "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
      "langchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed filetype-1.2.0 idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.0 pillow-avif-plugin-1.5.2 python-dotenv-1.1.1 roboflow-1.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:45:12.509850Z",
     "iopub.status.busy": "2025-09-09T14:45:12.509510Z",
     "iopub.status.idle": "2025-09-09T14:45:19.753822Z",
     "shell.execute_reply": "2025-09-09T14:45:19.753070Z",
     "shell.execute_reply.started": "2025-09-09T14:45:12.509826Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.3.1\n",
      "  Downloading scikit_learn-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.1) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.1) (2024.2.0)\n",
      "Downloading scikit_learn-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:45:19.755082Z",
     "iopub.status.busy": "2025-09-09T14:45:19.754787Z",
     "iopub.status.idle": "2025-09-09T14:45:54.420360Z",
     "shell.execute_reply": "2025-09-09T14:45:54.419616Z",
     "shell.execute_reply.started": "2025-09-09T14:45:19.755054Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.8.0\n",
      "  Downloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (1.4.8)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0) (2.9.0.post0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2,>=1.21->matplotlib==3.8.0) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.0) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.21->matplotlib==3.8.0) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2,>=1.21->matplotlib==3.8.0) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.21->matplotlib==3.8.0) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2,>=1.21->matplotlib==3.8.0) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2,>=1.21->matplotlib==3.8.0) (2024.2.0)\n",
      "Downloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.2\n",
      "    Uninstalling matplotlib-3.7.2:\n",
      "      Successfully uninstalled matplotlib-3.7.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:45:54.421769Z",
     "iopub.status.busy": "2025-09-09T14:45:54.421448Z",
     "iopub.status.idle": "2025-09-09T14:46:01.321056Z",
     "shell.execute_reply": "2025-09-09T14:46:01.320425Z",
     "shell.execute_reply.started": "2025-09-09T14:45:54.421732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.6.15)\n",
      "Requirement already satisfied: idna==3.7 in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.2.1)\n",
      "Requirement already satisfied: pi-heif<2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.0)\n",
      "Requirement already satisfied: pillow-avif-plugin<2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.58.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Blink-Classification-1 to folder:: 100%|██████████| 78789/78789 [00:00<00:00, 88517.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Blink-Classification-1 in folder:: 100%|██████████| 2911/2911 [00:00<00:00, 8146.43it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"B5AgQrixtU1h66djOZMm\")\n",
    "project = rf.workspace(\"digihack\").project(\"blink-classification-a5pan\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:46:01.322221Z",
     "iopub.status.busy": "2025-09-09T14:46:01.321984Z",
     "iopub.status.idle": "2025-09-09T14:46:01.326463Z",
     "shell.execute_reply": "2025-09-09T14:46:01.325623Z",
     "shell.execute_reply.started": "2025-09-09T14:46:01.322198Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dir = dataset.location + \"/train\"\n",
    "test_dir = dataset.location + \"/test\"\n",
    "val_dir = dataset.location + \"/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:46:01.327496Z",
     "iopub.status.busy": "2025-09-09T14:46:01.327287Z",
     "iopub.status.idle": "2025-09-09T14:46:02.424152Z",
     "shell.execute_reply": "2025-09-09T14:46:02.423573Z",
     "shell.execute_reply.started": "2025-09-09T14:46:01.327481Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689df3d28f384e009b09374196665cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/73.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    'mobilevitv2_200.cvnets_in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=2,\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "trans = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:46:02.425185Z",
     "iopub.status.busy": "2025-09-09T14:46:02.424959Z",
     "iopub.status.idle": "2025-09-09T14:46:02.430950Z",
     "shell.execute_reply": "2025-09-09T14:46:02.430070Z",
     "shell.execute_reply.started": "2025-09-09T14:46:02.425167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(data_dir)\n",
    "\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label  # Ensure this returns a tuple of (image, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:47:21.548950Z",
     "iopub.status.busy": "2025-09-09T14:47:21.548079Z",
     "iopub.status.idle": "2025-09-09T14:47:21.742848Z",
     "shell.execute_reply": "2025-09-09T14:47:21.742076Z",
     "shell.execute_reply.started": "2025-09-09T14:47:21.548920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 480, 480])\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((480,480)),\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset = CustomDataset(data_dir=train_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "for data in train_loader:\n",
    "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
    "    inputs, targets = data  # Unpack only if it has the correct structure\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T14:47:22.118657Z",
     "iopub.status.busy": "2025-09-09T14:47:22.118403Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/1] Training:  58%|█████▊    | 92/159 [01:23<01:06,  1.01it/s, loss=0.119] "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definisikan device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_val_loss = float('inf') \n",
    "save_path = \"./best_model.pth\"\n",
    "# Asumsikan model sudah didefinisikan\n",
    "# model = ...\n",
    "model.to(device) # Pindahkan model ke device\n",
    "\n",
    "# Definisikan loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)  # <-- TAMBAHKAN BARIS INI untuk memindahkan loss function ke GPU\n",
    "\n",
    "# Definisikan optimizer (setelah model dipindahkan ke device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "# --- 1. Buat DataLoader untuk Validasi ---\n",
    "# Letakkan ini di luar loop training, bersama dengan definisi train_loader Anda.\n",
    "try:\n",
    "    val_dataset = ImageFolder(root=val_dir, transform=transform)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "except NameError:\n",
    "    print(\"Pastikan variabel 'val_dir' dan 'transform' sudah didefinisikan.\")\n",
    "    # Atur val_loader ke None atau handle errornya agar tidak crash\n",
    "    val_loader = None\n",
    "\n",
    "\n",
    "num_epochs = 200 # Contoh jumlah epoch\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # ====================================================================\n",
    "    #                            TRAINING LOOP\n",
    "    # ====================================================================\n",
    "    model.train()  # Set the model to training mode\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\")\n",
    "    \n",
    "    for inputs, targets in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    # Hitung dan cetak metrik training setelah satu epoch selesai\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    accuracy = correct_predictions / total_samples * 100\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    train_metrics = (f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "                     f\"Train Loss: {avg_loss:.4f}, \"\n",
    "                     f\"Train Accuracy: {accuracy:.2f}%, \"\n",
    "                     f\"Train F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    print(train_metrics)\n",
    "    \n",
    "    # ====================================================================\n",
    "    #                           VALIDATION LOOP\n",
    "    # ====================================================================\n",
    "    if val_loader:\n",
    "        model.eval()  # PENTING: Ubah model ke mode evaluasi\n",
    "\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_val_targets = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad():  # PENTING: Nonaktifkan perhitungan gradien\n",
    "            val_progress_bar = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\")\n",
    "            for inputs, targets in val_progress_bar:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += (predicted == targets).sum().item()\n",
    "                \n",
    "                all_val_preds.extend(predicted.cpu().numpy())\n",
    "                all_val_targets.extend(targets.cpu().numpy())\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss  # Perbarui nilai loss terbaik\n",
    "            torch.save(model.state_dict(), save_path) # Simpan model state_dict\n",
    "            print(f\"✅ Model terbaik baru disimpan di epoch {epoch+1} dengan Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        # Hitung dan cetak metrik validasi\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total * 100\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(all_val_targets, all_val_preds, average='weighted', zero_division=0)\n",
    "\n",
    "        val_metrics = (f\"             \" # Spasi untuk meluruskan\n",
    "                       f\"Val Loss: {avg_val_loss:.4f},  \"\n",
    "                       f\"Val Accuracy: {val_accuracy:.2f}%,  \"\n",
    "                       f\"Val F1-Score: {val_f1:.4f}\\n\") # Tambah baris baru\n",
    "        \n",
    "        print(val_metrics)\n",
    "torch.save(model.state_dict(), \"./last_model.pth\") # Simpan model state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "test_dataset = ImageFolder(root=test_dir, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 4. Define testing loop\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "\n",
    "\n",
    "# Perform forward pass\n",
    "#outputs = model(inputs)\n",
    "\n",
    "# Get predicted class indices\n",
    "#_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "#correct_predictions += (preds == targets).sum().item()\n",
    "#total_samples += targets.size(0)\n",
    "\n",
    "# Store predictions and labels for metrics\n",
    "#all_targets.extend(targets.tolist())\n",
    "#all_preds.extend(preds.tolist())\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct_predictions += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        all_targets.extend(targets.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
    "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "report = classification_report(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "results_file = f\"Tiny test2.txt\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
    "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
    "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
    "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(conf_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model weights from ./best_model.pth and prepare for inference\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Recreate the exact model architecture used in training\n",
    "model_name = 'mobilevitv2_200.cvnets_in1k'\n",
    "num_classes = 2\n",
    "\n",
    "# Reuse existing device if defined, otherwise detect automatically\n",
    "device = globals().get('device', torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Build base model (no need to download pretrained weights when loading a checkpoint)\n",
    "base_model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "\n",
    "ckpt_path = './best_model.pth'\n",
    "if not os.path.exists(ckpt_path):\n",
    "    raise FileNotFoundError(f'Checkpoint not found at {ckpt_path}. Make sure training saved it successfully.')\n",
    "\n",
    "# Load checkpoint (supports both raw state_dict and dict with key \"state_dict\")\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "state_dict = state.get('state_dict', state) if isinstance(state, dict) else state\n",
    "\n",
    "# Remove potential DataParallel 'module.' prefix if present\n",
    "if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "    cleaned_state_dict = OrderedDict((k.replace('module.', '', 1), v) for k, v in state_dict.items())\n",
    "else:\n",
    "    cleaned_state_dict = state_dict\n",
    "\n",
    "# Load weights (strict=False to allow minor non-critical mismatches)\n",
    "missing_keys, unexpected_keys = base_model.load_state_dict(cleaned_state_dict, strict=False)\n",
    "\n",
    "# Optionally wrap with DataParallel if multiple GPUs are available\n",
    "model = base_model\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(base_model)\n",
    "\n",
    "model = model.to(device).eval()\n",
    "\n",
    "print(f'Successfully loaded weights from {ckpt_path} onto {device}.')\n",
    "if missing_keys:\n",
    "    print('Missing keys:', missing_keys)\n",
    "if unexpected_keys:\n",
    "    print('Unexpected keys:', unexpected_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "test_dataset = ImageFolder(root=test_dir, transform = transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 4. Define testing loop\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "\n",
    "\n",
    "# Perform forward pass\n",
    "#outputs = model(inputs)\n",
    "\n",
    "# Get predicted class indices\n",
    "#_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "# Compare predictions with ground truth\n",
    "#correct_predictions += (preds == targets).sum().item()\n",
    "#total_samples += targets.size(0)\n",
    "\n",
    "# Store predictions and labels for metrics\n",
    "#all_targets.extend(targets.tolist())\n",
    "#all_preds.extend(preds.tolist())\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        correct_predictions += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "        all_targets.extend(targets.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "precision = precision_score(all_targets, all_preds, average=None, zero_division=0)\n",
    "recall = recall_score(all_targets, all_preds, average=None, zero_division=0)\n",
    "conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "report = classification_report(all_targets, all_preds, zero_division=0)\n",
    "\n",
    "results_file = f\"Tiny test2.txt\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    f.write(f\"\\nPrecision (per class): {precision}\\n\")\n",
    "    f.write(f\"\\nRecall (per class): {recall}\\n\")\n",
    "    f.write(f\"\\nConfusion Matrix:\\n{conf_matrix}\\n\")\n",
    "    f.write(f\"\\nClassification Report:\\n{report}\\n\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(conf_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
