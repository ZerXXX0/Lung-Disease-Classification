{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290fc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import timm\n",
    "import torch\n",
    "import zipfile,os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apakan sama path di pc bay\n",
    "\n",
    "train_dir = \"/kaggle/input/lung-disease/train/train\"\n",
    "val_dir = \"/kaggle/input/lung-disease/val/val\"\n",
    "test_dir = \"/kaggle/input/lung-disease/test/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be42aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'maxvit_rmlp_base_rw_224.sw_in12k_ft_in1k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=5,\n",
    ")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "# Get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "print(f\"Model expects input size: {data_config['input_size']}\")\n",
    "print(f\"Model normalization: mean={data_config['mean']}, std={data_config['std']}\")\n",
    "\n",
    "# Create training and validation transforms\n",
    "train_transform = timm.data.create_transform(**data_config, is_training=True)\n",
    "val_transform = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c2106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Get class names and sort them for consistency\n",
    "        self.class_names = sorted(os.listdir(data_dir))\n",
    "        print(f\"Found classes: {self.class_names}\")\n",
    "\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            if os.path.isdir(class_dir):  # Only process directories\n",
    "                class_images = [f for f in os.listdir(class_dir) \n",
    "                              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "                print(f\"Class '{class_name}': {len(class_images)} images\")\n",
    "                \n",
    "                for img_name in class_images:\n",
    "                    img_path = os.path.join(class_dir, img_name)\n",
    "                    self.images.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        print(f\"Total images loaded: {len(self.images)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            label = self.labels[idx]\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            # Return a black image if loading fails\n",
    "            if self.transform:\n",
    "                black_image = self.transform(Image.new('RGB', (224, 224), (0, 0, 0)))\n",
    "            else:\n",
    "                black_image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "            return black_image, self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4939f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data diagnostic\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create datasets with appropriate transforms\n",
    "train_dataset = CustomDataset(data_dir=train_dir, transform=train_transform)\n",
    "val_dataset = CustomDataset(data_dir=val_dir, transform=val_transform)\n",
    "\n",
    "# Create data loaders with NO workers for Kaggle compatibility\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=True, \n",
    "    num_workers=0,  # Set to 0 for Kaggle/Colab compatibility\n",
    "    pin_memory=False,  # Disable pin_memory for stability\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=64,\n",
    "    shuffle=False, \n",
    "    num_workers=0,  # Set to 0 for Kaggle/Colab compatibility\n",
    "    pin_memory=False  # Disable pin_memory for stability\n",
    ")\n",
    "\n",
    "print(\"=== Data Loading Test ===\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test data loading and show sample\n",
    "try:\n",
    "    for data in train_loader:\n",
    "        inputs, targets = data\n",
    "        print(f\"✅ Training batch shape: {inputs.shape}\")\n",
    "        print(f\"✅ Training labels shape: {targets.shape}\")\n",
    "        print(f\"✅ Input range: [{inputs.min().item():.3f}, {inputs.max().item():.3f}]\")\n",
    "        print(f\"✅ Label range: [{targets.min().item()}, {targets.max().item()}]\")\n",
    "        print(f\"✅ Unique labels in batch: {torch.unique(targets).tolist()}\")\n",
    "        \n",
    "        # Show a few sample images\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "        for i in range(min(4, inputs.shape[0])):\n",
    "            img = inputs[i]\n",
    "            # Denormalize for display\n",
    "            mean = torch.tensor(data_config['mean']).view(3, 1, 1)\n",
    "            std = torch.tensor(data_config['std']).view(3, 1, 1)\n",
    "            img = img * std + mean\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "            \n",
    "            axes[i].imshow(img.permute(1, 2, 0))\n",
    "            axes[i].set_title(f\"Label: {targets[i].item()}\\nClass: {train_dataset.class_names[targets[i].item()]}\")\n",
    "            axes[i].axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in data loading: {e}\")\n",
    "    print(\"This might be the source of your training issues!\")\n",
    "\n",
    "print(\"\\n=== Data Loaders Ready ===\")\n",
    "print(\"✅ Your dataset has 5 lung disease classes!\")\n",
    "print(\"✅ Model updated to 5 classes - now everything should work!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757989a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check data distribution first\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Training classes: {train_dataset.class_names}\")\n",
    "print(f\"Validation classes: {val_dataset.class_names}\")\n",
    "\n",
    "# Count class distribution\n",
    "train_class_counts = {}\n",
    "for label in train_dataset.labels:\n",
    "    class_name = train_dataset.class_names[label]\n",
    "    train_class_counts[class_name] = train_class_counts.get(class_name, 0) + 1\n",
    "print(f\"Training class distribution: {train_class_counts}\")\n",
    "\n",
    "val_class_counts = {}\n",
    "for label in val_dataset.labels:\n",
    "    class_name = val_dataset.class_names[label]\n",
    "    val_class_counts[class_name] = val_class_counts.get(class_name, 0) + 1\n",
    "print(f\"Validation class distribution: {val_class_counts}\")\n",
    "\n",
    "# Setup device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device and ensure it's in training mode\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Define loss function with class weights for imbalanced data\n",
    "total_train = len(train_dataset)\n",
    "class_weights = []\n",
    "for class_name in train_dataset.class_names:\n",
    "    class_count = train_class_counts.get(class_name, 1)\n",
    "    weight = total_train / (len(train_dataset.class_names) * class_count)\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Optimizer with lower learning rate\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "save_path = \"./best_model.pth\"\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"\\n=== Starting Training ===\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ====================================================================\n",
    "    #                            TRAINING LOOP\n",
    "    # ====================================================================\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Training\")\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += targets.size(0)\n",
    "        correct_predictions += (predicted == targets).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_acc = 100.0 * correct_predictions / total_samples\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{current_acc:.1f}%'\n",
    "        })\n",
    "    \n",
    "    # Calculate training metrics\n",
    "    avg_train_loss = epoch_loss / len(train_loader)\n",
    "    train_accuracy = 100.0 * correct_predictions / total_samples\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Training:\")\n",
    "    print(f\"  Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, F1: {f1:.4f}\")\n",
    "    \n",
    "    # ====================================================================\n",
    "    #                           VALIDATION LOOP\n",
    "    # ====================================================================\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    all_val_targets = []\n",
    "    all_val_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_progress_bar = tqdm(val_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] Validation\")\n",
    "        for inputs, targets in val_progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += targets.size(0)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "            \n",
    "            all_val_preds.extend(predicted.cpu().numpy())\n",
    "            all_val_targets.extend(targets.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_val_acc = 100.0 * val_correct / val_total\n",
    "            val_progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{current_val_acc:.1f}%'\n",
    "            })\n",
    "    \n",
    "    # Calculate validation metrics\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100.0 * val_correct / val_total\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "        all_val_targets, all_val_preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Validation:\")\n",
    "    print(f\"  Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  ✅ Best model saved! Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Early stopping if learning rate gets too small\n",
    "    if current_lr < 1e-7:\n",
    "        print(\"Learning rate too small, stopping training.\")\n",
    "        break\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"./last_model.pth\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy', marker='o')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy', marker='s')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8223eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import timm\n",
    "\n",
    "# ==============================\n",
    "# CUSTOM DATASET WITH PATHS\n",
    "# ==============================\n",
    "class ImageFolderWithPaths(ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths.\"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        # Normal ImageFolder return (img, label)\n",
    "        original_tuple = super().__getitem__(index)\n",
    "        path = self.samples[index][0]  # file path\n",
    "        return original_tuple + (path,)  # (img, label, path)\n",
    "\n",
    "# ==============================\n",
    "# DATASET & LOADER\n",
    "# ==============================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = ImageFolderWithPaths(root=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ==============================\n",
    "# DEVICE\n",
    "# ==============================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ==============================\n",
    "# MODEL\n",
    "# ==============================\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=5,\n",
    ")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# ==============================\n",
    "# LOAD BEST MODEL\n",
    "# ==============================\n",
    "state_dict = torch.load(\"./best_model.pth\", map_location=device)\n",
    "if isinstance(model, nn.DataParallel):\n",
    "    model.module.load_state_dict(state_dict)\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "print(\"✅ Loaded best_model.pth successfully.\")\n",
    "\n",
    "# ==============================\n",
    "# PREDICTION LOOP\n",
    "# ==============================\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, _, paths in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        for path, pred in zip(paths, preds.cpu().numpy()):\n",
    "            filename = os.path.basename(path)\n",
    "\n",
    "            # ✅ Clean up Roboflow suffix like \".rf.xxxxx\"\n",
    "            if \".rf.\" in filename:\n",
    "                filename = filename.split(\".rf.\")[0]\n",
    "\n",
    "            # ✅ Replace \"_jpeg\" suffix with \".jpeg\"\n",
    "            if filename.endswith(\"_jpeg\"):\n",
    "                filename = filename.replace(\"_jpeg\", \".jpeg\")\n",
    "\n",
    "            results.append([filename, pred])\n",
    "\n",
    "# ==============================\n",
    "# SAVE CSV\n",
    "# ==============================\n",
    "csv_file = \"test_predictions.csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Id\", \"Predicted\"])  # header\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"✅ Predictions saved to {csv_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
